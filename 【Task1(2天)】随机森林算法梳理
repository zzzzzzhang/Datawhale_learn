1. 集成学习概念
“三个臭皮匠，顶个诸葛亮”
“弱可学习”等价于“强可学习” —— Schapire【1】
可以理解为就是指采用多个分类器对数据集进行预测，从而提高整体分类器的泛化能力。 
集成学习有两个流派
一个是boosting派系,它的特点是各个弱学习器之间有依赖关系。
另一种是bagging流派,它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。

2. 个体学习器概念
又称“基学习器”，感觉只有在集成学习里提到，可以是决策树、神经网络、朴素贝叶斯法、支持向量机等等
单个学习器可能效果并不好，所以需要集成学习

3. boosting 
可将弱学习器提升为强学习器的算法
前一个学习器学习错误的样本在下次学习中将会得到“特殊关照”，拟合残差思想
代表性算法：Adaboost
以m个样本的二分类为例,初始每个样本的权重W_mi为1/m
训练一个基学习器T0，计算错误率err = S W_mi*[G(xi)≠yi]
计算该学习器的权重W_T0 = 1/2 * (ln(1-err)/err)
重新计算每个样本的权重:分类对的W_mi（新一轮） = W_mi(上一轮)*exp（-W_T0）/sum_W 
重新计算每个样本的权重:分类错的W_mi（新一轮） = W_mi(上一轮)*exp（W_T0）/sum_W
再根据新的训练集训练基学习器T1
如此循环

4. bagging
又称装袋算法，算法思想是训练多个“好而不同”的基学习器进行投票决定该样本属于哪个类（回归的话，输出各学习器输出的平均值）
假如有m个样本,有放回选取m个集合D，每次不被选到的概率为（1-1/m）,当m趋于无穷时候，不被选到的概率为1/e,约36.8%
不被称为袋外数据,可以用于交叉验证
用集合D训练基学习器T0
如此反复,得到N个基学习器,集成为强学习器,通过投票、平均法（回归）输出最终结果

5. 结合策略(平均法，投票法，学习法)
平均法：
简单平均、加权平均
个体学习器性能相差较大时宜使用加权平均法,相近用简单平均法。加权平均容易过拟合
投票法：
相对多数投票法：输出得票最多的预测
绝对多数投票法：得票最多超过50%的输出,否则拒绝预测
加权投票法：给每个分类器预测结果一个权重再求和,最大的输出

6. 随机森林思想
随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于集成学习方法。
随机森林的名称中有两个关键词，一个是“随机”，一个就是“森林”。
“森林”我们很好理解，一棵叫做树，那么成百上千棵就可以叫做森林了，这也是随机森林的主要思想--集成思想的体现。
然而，bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以bagging改进了预测准确率但损失了解释性。
“森林”容易理解，就是由很多“树”组成，那么“随机”体现在什么方面呢？
（1）训练集随机的选取：如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），
作为该树的训练集；这样保证了每颗树的训练集都不同，从而构建的树也不同
（2）特征的随机选取：从M个特征中选取m个特征，这样可以避免某个特征与分类结果具有强相关性，
如果所有特征都选取，那么所有的树都会很相似，那样就不够“随机”了
另外还有一点，随机森林法构建树的时候不需要做额外的剪枝操作。
个人理解：因为前两个“随机”操作，以及多颗树的建立，已经避免了过拟合现象，所以这种情况下，我们只需要让每棵树在它自己的领域内做到最好就可以了。

随机森林算法的预测性能与两个因素有关：
（1）森林中任意两棵树的相关性，相关性越强，则总体性能越容易差
（2）森林中每棵树的预测性能，每棵树越好，则总体性能越好
其实可以理解为要求“好而不同”。然而特征数m的选择越大，则相关性与个体性能都比较好，
特征数m选择越小，则相关性与个体性能都更小，所以m的选择影响着随机森林的预测性能。


7. 随机森林的推广
（1）extra trees：样本集不抽样，直接使用原样本集。分裂的时候随机选择特征，相较于RF，每棵树的规模更大，但是进一步减小了方差，增加了bias
（2）Totally Random Trees Embedding：根据拥有某特征的样本x在不同树的叶子结点号，给数据升维


8. 优缺点
优点：
  （1） 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。
　（2） 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
　（3） 在训练后，可以给出各个特征对于输出的重要性
　（4） 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
　（5） 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
　（6） 对部分特征缺失不敏感。
缺点：
  （1）随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合
  （2）对小样本数据或者低维（特征少）数据可能不能产生最好的分类

9. sklearn参数
待添加

10.应用场景
（1）数据维度不是很高（也不能太低..毕竟要随机抽取特征），而对准确性有较高要求时。
（2）因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。
