1. 集成学习概念
“三个臭皮匠，顶个诸葛亮”
“弱可学习”等价于“强可学习” —— Schapire【1】
可以理解为就是指采用多个分类器对数据集进行预测，从而提高整体分类器的泛化能力。 
集成学习有两个流派
一个是boosting派系,它的特点是各个弱学习器之间有依赖关系。
另一种是bagging流派,它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。

2. 个体学习器概念
又称“基学习器”，感觉只有在集成学习里提到，可以是决策树、神经网络、朴素贝叶斯法、支持向量机等等
单个学习器可能效果并不好，所以需要集成学习

3. boosting 
可将弱学习器提升为强学习器的算法
前一个学习器学习错误的样本在下次学习中将会得到“特殊关照”，拟合残差思想
代表性算法：Adaboost
以m个样本的二分类为例,初始每个样本的权重W_mi为1/m
训练一个基学习器T0，计算错误率err = S W_mi*[G(xi)≠yi]
计算该学习器的权重W_T0 = 1/2 * (ln(1-err)/err)
重新计算每个样本的权重:分类对的W_mi（新一轮） = W_mi(上一轮)*exp（-W_T0）/sum_W 
重新计算每个样本的权重:分类错的W_mi（新一轮） = W_mi(上一轮)*exp（W_T0）/sum_W
再根据新的训练集训练基学习器T1
如此循环

4. bagging
又称装袋算法，算法思想是训练多个“好而不同”的基学习器进行投票决定该样本属于哪个类（回归的话，输出各学习器输出的平均值）
假如有m个样本,有放回选取m个集合D，每次不被选到的概率为（1-1/m）,当m趋于无穷时候，不被选到的概率为1/e,约36.8%
不被称为袋外数据,可以用于交叉验证
用集合D训练基学习器T0
如此反复,得到N个基学习器,集成为强学习器,通过投票、平均法（回归）输出最终结果

5. 结合策略(平均法，投票法，学习法)
平均法：
简单平均、加权平均
个体学习器性能相差较大时宜使用加权平均法,相近用简单平均法。加权平均容易过拟合
投票法：
相对多数投票法：输出得票最多的预测
绝对多数投票法：得票最多超过50%的输出,否则拒绝预测
加权投票法：给每个分类器预测结果一个权重再求和,最大的输出

6. 随机森林思想
7. 随机森林的推广
8. 优缺点
9. sklearn参数
10.应用场景
